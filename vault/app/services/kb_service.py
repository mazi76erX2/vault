from __future__ import annotations

import logging
import uuid

from sqlalchemy import delete, func, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.integrations.ollama_client import embed
from app.models.kb import KBChunk, KBDocument

logger = logging.getLogger(__name__)

async def store_in_kb(data: dict, db: AsyncSession) -> dict:
    """Same as store_bulk but for a single chunk/doc."""
    try:
        # Check if doc exists or create new KBDocument
        # For simplicity, we create a new Document for each upload call usually
        # But data['content'] here is just text.
        # The 'data' dict structure from knowledge_base.py seems to be chunk-oriented if len(chunks)==1

        doc_id = uuid.uuid4()

        document = KBDocument(
            id=doc_id,
            content=data.get("content", ""),
            sourcefile=data.get("file_name"),
            title=data.get("file_title"),
            accesslevel=data.get("level", 1),
            companyid=data.get("company_id"),
            department=data.get("department"),
            # tsv will be auto-generated by DB trigger if set up, or we can set it explicitly if model handles it
        )
        db.add(document)
        await db.flush() # get ID

        # Generator embedding
        embedding = embed(data.get("content", ""))

        chunk = KBChunk(
            id=uuid.uuid4(),
            doc_id=doc_id,
            chunk_index=0,
            content=data.get("content", ""),
            embedding=embedding,
            title=data.get("file_title"),
            sourcefile=data.get("file_name"),
            accesslevel=data.get("level", 1)
        )
        db.add(chunk)
        await db.commit()

        return {"doc_id": str(doc_id), "chunk_id": str(chunk.id)}

    except Exception as e:
        await db.rollback()
        logger.error(f"Error storing in KB: {e}")
        raise e

async def store_bulk_in_kb(docs_list: list[dict], db: AsyncSession) -> dict:
    """Store multiple chunks (usually from one file)."""
    try:
        if not docs_list:
            return {"doc_ids": []}

        # Assuming all chunks belong to the same file/document logically,
        # but the list structure passed from knowledge_base.py treats them as separate dicts.
        # We need to create ONE Parent KBDocument and multiple KBChunks.

        first = docs_list[0]
        parent_doc_id = uuid.uuid4()

        # Aggregate content for the parent document (optional, or just first chunk?)
        # Usually KBDocument represents the whole file.
        # We'll just create the KBDocument container.

        full_content = "\n".join([d.get("content", "") for d in docs_list])

        document = KBDocument(
            id=parent_doc_id,
            content=full_content, # Store full text in parent
            sourcefile=first.get("file_name"),
            title=first.get("file_name"), # Use filename as title base
            accesslevel=first.get("level", 1),
            companyid=first.get("company_id"),
            department=first.get("department"),
        )
        db.add(document)
        await db.flush()

        # Process chunks
        chunk_objects = []

        # Get embeddings in bulk if possible, else loop
        # embed() might accept list? check ollama_client
        # For now loop

        for i, d in enumerate(docs_list):
            content = d.get("content", "")
            embedding = embed(content) # Sync call?

            chunk = KBChunk(
                id=uuid.uuid4(),
                doc_id=parent_doc_id,
                chunk_index=i,
                content=content,
                embedding=embedding,
                title=d.get("file_title"),
                sourcefile=d.get("file_name"),
                accesslevel=d.get("level", 1)
            )
            chunk_objects.append(chunk)

        db.add_all(chunk_objects)
        await db.commit()

        return {"doc_ids": [str(parent_doc_id)], "chunk_count": len(chunk_objects)}

    except Exception as e:
        await db.rollback()
        logger.error(f"Error bulk storing in KB: {e}")
        raise e

async def delete_from_kb(doc_id: str, db: AsyncSession) -> dict:
    try:
        # Cascade delete should handle chunks if configured in models,
        # but let's be explicit or trust FK
        stmt = delete(KBDocument).where(KBDocument.id == doc_id)
        result = await db.execute(stmt)
        await db.commit()

        if result.rowcount > 0:
            return {"status": "success", "id": doc_id}
        else:
            return {"status": "not_found", "id": doc_id}

    except Exception as e:
        await db.rollback()
        logger.error(f"Error deleting from KB: {e}")
        return {"status": "error", "error": str(e)}

async def list_documents(
    db: AsyncSession,
    limit: int = 50,
    offset: int = 0,
    access_level: int = 1,
    company_id: int = None
) -> list[dict]:
    try:
        stmt = select(KBDocument).where(
            KBDocument.accesslevel <= access_level
        )

        if company_id:
            stmt = stmt.where(KBDocument.companyid == company_id)

        stmt = stmt.order_by(KBDocument.createdat.desc()).limit(limit).offset(offset)

        result = await db.execute(stmt)
        rows = result.scalars().all()

        return [
            {
                "id": str(r.id),
                "title": r.title,
                "created_at": r.createdat,
                "source": r.sourcefile
            }
            for r in rows
        ]
    except Exception as e:
        logger.error(f"List docs error: {e}")
        return []

async def get_document_count(
    db: AsyncSession,
    access_level: int = 1,
    company_id: int = None
) -> int:
    try:
        stmt = select(func.count(KBDocument.id)).where(
            KBDocument.accesslevel <= access_level
        )
        if company_id:
            stmt = stmt.where(KBDocument.companyid == company_id)

        result = await db.execute(stmt)
        return result.scalar() or 0
    except Exception:
        return 0

async def search_kb(
    db: AsyncSession,
    query: str,
    limit: int = 5,
    access_level: int = 1,
    company_id: int = None,
    company_reg_no: str = None,
    similarity_threshold: float = 0.0
) -> list[dict]:
    """Search knowledge base using RAG Pipeline (Hybrid Search + Reranking)."""
    from app.services.rag_pipeline import get_rag_pipeline
    pipeline = get_rag_pipeline()

    # Build filters
    filters = {}
    if access_level:
        filters["accesslevel"] = access_level
    if company_id:
        filters["companyid"] = company_id

    # Execute retrieval
    ranked_docs = await pipeline.retrieve(
        db,
        query,
        filters=filters,
        top_k=limit
    )

    return [
        {
            "title": doc.metadata.get("title", "Untitled"),
            "content": doc.content,
            "score": doc.score,
            "id": doc.id,
            "source": doc.metadata.get("source", "")
        }
        for doc in ranked_docs
    ]
